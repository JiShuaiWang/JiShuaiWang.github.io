<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HDFS原理及操作]]></title>
    <url>%2F2019%2F11%2F01%2Fhadoop-learn-hdfs%2F</url>
    <content type="text"><![CDATA[HDFS原理HDFS（Hadoop Distributed File System）是一个分布式文件系统。它具有高容错性并提供了高吞吐量的数据访问，非常适合大规模数据集上的应用，它提供了一个高度容错性和高吞吐量的海量数据存储解决方案。 高吞吐量访问：HDFS的每个Block分布在不同的Rack上，在用户访问时，HDFS会计算使用最近和访问量最小的服务器给用户提供。由于Block在不同的Rack上都有备份，所以不再是单数据访问，所以速度和效率是非常快的。另外HDFS可以并行从服务器集群中读写，增加了文件读写的访问带宽。 高容错性：系统故障是不可避免的，如何做到故障之后的数据恢复和容错处理是至关重要的。HDFS通过多方面保证数据的可靠性，多份复制并且分布到物理位置的不同服务器上，数据校验功能、后台的连续自检数据一致性功能都为高容错提供了可能。 线性扩展：因为HDFS的Block信息存放到NameNode上，文件的Block分布到DataNode上，当扩充的时候仅仅添加DataNode数量，系统可以在不停止服务的情况下做扩充，不需要人工干预。 HDFS架构如上图所示HDFS是Master和Slave的结构，分为NameNode、Secondary NameNode和DataNode三种角色。 NameNode：在Hadoop1.X中只有一个Master节点，管理HDFS的名称空间和数据块映射信息、配置副本策略和处理客户端请求；Secondary NameNode：辅助NameNode，分担NameNode工作，定期合并fsimage和fsedits并推送给NameNode，紧急情况下可辅助恢复NameNode；DataNode：Slave节点，实际存储数据、执行数据块的读写并汇报存储信息给NameNode； Block数据块; 基本存储单位，一般大小为64M（配置大的块主要是因为：1）减少搜寻时间，一般硬盘传输速率比寻道时间要快，大的块可以减少寻道时间；2）减少管理块的数据开销，每个块都需要在NameNode上有对应的记录；3）对数据块进行读写，减少建立网络的连接成本） 一个大文件会被拆分成一个个的块，然后存储于不同的机器。如果一个文件少于Block大小，那么实际占用的空间为其文件的大小 基本的读写单位，类似于磁盘的页，每次都是读写一个块 每个块都会被复制到多台机器，默认复制3份NameNode 存储文件的metadata，运行时所有数据都保存到内存，整个HDFS可存储的文件数受限于NameNode的内存大小 一个Block在NameNode中对应一条记录（一般一个block占用150字节），如果是大量的小文件，会消耗大量内存。同时map task的数量是由splits来决定的，所以用MapReduce处理大量的小文件时，就会产生过多的map task，线程管理开销将会增加作业时间。处理大量小文件的速度远远小于处理同等大小的大文件的速度。因此Hadoop建议存储大文件 数据会定时保存到本地磁盘，但不保存block的位置信息，而是由DataNode注册时上报和运行时维护（NameNode中与DataNode相关的信息并不保存到NameNode的文件系统中，而是NameNode每次重启后，动态重建） NameNode失效则整个HDFS都失效了，所以要保证NameNode的可用性Secondary NameNode 定时与NameNode进行同步（定期合并文件系统镜像和编辑日志，然后把合并后的传给NameNode，替换其镜像，并清空编辑日志，类似于CheckPoint机制），但NameNode失效后仍需要手工将其设置成主机DataNode 保存具体的block数据 负责数据的读写操作和复制操作 DataNode启动时会向NameNode报告当前存储的数据块信息，后续也会定时报告修改信息 DataNode之间会进行通信，复制数据块，保证数据的冗余性 Hadoop 写文件 1.客户端将文件写入本地磁盘的HDFS Client文件中 2.当临时文件大小达到一个block大小时，HDFS client通知NameNode，申请写入文件 3.NameNode在HDFS的文件系统中创建一个文件，并把该block id和要写入的DataNode的列表返回给客户端 4.客户端收到这些信息后，将临时文件写入DataNodes 4.1 客户端将文件内容写入第一个DataNode（一般以4kb为单位进行传输）4.2 第一个DataNode接收后，将数据写入本地磁盘，同时也传输给第二个DataNode4.3 依此类推到最后一个DataNode，数据在DataNode之间是通过pipeline的方式进行复制的4.4 后面的DataNode接收完数据后，都会发送一个确认给前一个DataNode，最终第一个DataNode返回确认给客户端4.5 当客户端接收到整个block的确认后，会向NameNode发送一个最终的确认信息4.6 如果写入某个DataNode失败，数据会继续写入其他的DataNode。然后NameNode会找另外一个好的DataNode继续复制，以保证冗余性4.7 每个block都会有一个校验码，并存放到独立的文件中，以便读的时候来验证其完整性5.文件写完后（客户端关闭），NameNode提交文件（这时文件才可见，如果提交前，NameNode垮掉，那文件也就丢失了。fsync：只保证数据的信息写到NameNode上，但并不保证数据已经被写到DataNode中） Rack aware（机架感知） 通过配置文件指定机架名和DNS的对应关系 假设复制参数是3，在写入文件时，会在本地的机架保存一份数据，然后在另外一个机架内保存两份数据（同机架内的传输速度快，从而提高性能） 整个HDFS的集群，最好是负载平衡的，这样才能尽量利用集群的优势 Hadoop 读文件 客户端向NameNode发送读取请求 NameNode返回文件的所有block和这些block所在的DataNodes（包括复制节点） 客户端直接从DataNode中读取数据，如果该DataNode读取失败（DataNode失效或校验码不对），则从复制节点中读取（如果读取的数据就在本机，则直接读取，否则通过网络读取）Hadoop 可靠性HDFS - 可靠性 DataNode可以失效 DataNode会定时发送心跳到NameNode。如果一段时间内NameNode没有收到DataNode的心跳消息，则认为其失效。此时NameNode就会将该节点的数据（从该节点的复制节点中获取）复制到另外的DataNode中 数据可以毁坏无论是写入时还是硬盘本身的问题，只要数据有问题（读取时通过校验码来检测），都可以通过其他的复制节点读取，同时还会再复制一份到健康的节点中 NameNode不可靠HDFS中常用到的命令 hadoop fs12345678910111213hadoop fs -ls / hadoop fs -lsr hadoop fs -mkdir /user/hadoop hadoop fs -put a.txt /user/hadoop/ hadoop fs -get /user/hadoop/a.txt / hadoop fs -cp src dst hadoop fs -mv src dst hadoop fs -cat /user/hadoop/a.txt hadoop fs -rm /user/hadoop/a.txt hadoop fs -rmr /user/hadoop/a.txt hadoop fs -text /user/hadoop/a.txt hadoop fs -copyFromLocal localsrc dst 与hadoop fs -put功能类似。 hadoop fs -moveFromLocal localsrc dst]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 语法帮助]]></title>
    <url>%2F2019%2F10%2F30%2FMarkdown-help%2F</url>
    <content type="text"><![CDATA[markdown - 更简洁、更高效================================================= 强烈建议开发者认真阅读本文档，掌握md及HBuilderX对md的强大支持。窄屏幕下，可按Alt+滚轮横向滚动 很多人只把markdown用于网络文章发表，这糟蹋了markdown。markdown不止是HTML的简化版，更重要的是txt的升级版、word的轻量版、笔记的最佳载体。作为一种简单的格式标记语言，不同于txt的无格式，不同于HTML的复杂标记，也不同于word的鼠标调整样式。markdown通过简单的几个字符键入，就可以快捷的定义文档的样式。比如在行首敲一个“#”，就把这行定义为了1级标题，并且在HBuilderX里有直观完善的着色，这样无需发布为web页面，可直接当word用。掌握markdown，你可以完全抛弃txt和笔记软件的编辑器，并且在大多数场景下替代掉复杂臃肿的word。享受简洁之美、享受效率提升。而HBuilderX，可以被称为最强大的markdown书写工具了。 下面的示例列举了markdown语法及对应的HBuilderX使用技巧： 开始前，可以先按下文档结构图的快捷键Alt+w（Mac是Ctrl+w），浏览本文的大纲。 标题语法markdown的标题是行首以#号开头，空格分割的，不同级别的标题，在HX里着色也不同。如下： 标题1标题2标题3标题4标题5标题6标题使用技巧： Emmet快速输入：敲h2+Tab即可生成二级标题【同HTML里的emmet写法，不止标题，HX里所有可对应tag的markdown语法均支持emmet写法】。仅行首生效 智能双击：双击#号可选中整个标题段落 智能回车：行尾回车或行中Ctrl+Enter强制换行后会自动在下一行补#。而连续2次回车后将自动补的#去掉。(体验同word) 回车后再次按Tab可递进一层标题，再按Tab切换列表符 在# 后回车，可上插一个空标题行【同word】，或任意位置按Ctrl+Shift+Enter也可以上插空标题行 折叠： 点标题前的-号可折叠该标题段落，快捷键是Alt+-（展开折叠是Alt+=） 多层折叠时折叠或展开子节点，快捷键是Alt+Shift+-或= 全文全部折叠或展开，快捷键是Ctrl+Alt+Shift+-或= 折叠其他区域，快捷键是Alt+Shift+o。这对长文档管理非常有用，可以专注于当前章节 可以在菜单-跳转-折叠中随时找到这些功能 列表markdown的列表支持有序列表、无序列表以及特殊的任务列表。同样也是在行前加一个特殊符号，并空格后再跟列表文字内容。 有序列表有序列表就是有顺序的列表，依靠行前的数字标记顺序。 有序列表1 【设置或取消有序列表符的快捷键：Ctrl+Alt+1，可选中多行批量设置序号；支持多光标批量设置列表符，即按Ctrl+鼠标左键添加多光标】 有序列表2 【列表后回车会自动补序号】 有序列表3 【智能双击：双击前面的数字，可重新对数字排序，修正序号错误，并选中有序列表段落（左边的4是故意写错让你体验的）】 无序列表无序列表就是列表不排序，无序列表因书写随意而被更广泛的使用。无序列表有3种前缀，HX里分别用于表示1级列表、2级列表、3级列表。 无序列表1 【快捷键：Ctrl+Alt+-；智能双击：双击-号可选中整段无序列表；再次按Tab会更换二级列表符】 无序列表2 Emmet：li后敲Tab可生成*号列表符，行首生效 快捷键：Ctrl+Alt+8【8即*对应的数字】，支持多光标批量设置列表符，即按Ctrl+鼠标左键添加多光标 智能双击：双击*号可选中整段无序列表 智能回车：行尾回车或行中Ctrl+Enter强制换行后会自动续列表；连续按回车会清除列表符；再次按Tab会更换列表符；在列表符后回车或行尾Shift+回车，上一行留出列表符 *号常用于二级列表，列表符后继续Tab，可切换列表符 无序列表3 【快捷键：Ctrl+Alt+=；常用于三级列表；其他同上】 任务列表任务列表非常实用，管理待办、已办非常便利。[ ] 任务列表-未完成任务 【快捷键：Ctrl+Alt+[】[x] 任务列表-已完成任务 【快捷键：Ctrl+Alt+]】 1. 智能双击：双击方括号内可切换勾选状态，把任务标记为完成或未完成；双击方括号右侧可选中任务列表段落 2. 智能回车：回车后自动补任务列表前缀符号；连续按回车清除前缀符号；在列表符后回车或行尾Shift+回车，上一行留出列表符 如需发布到web渲染，需增加无序列表- 的前缀 以上三种列表，均支持批量修改列表符，有如下方式建议依次学习尝试： 选中多行，按快捷键Ctrl+Alt+“1”或“-”或“[”或“]”，批量设置列表符 如果需要跳行设置有序或无序列表，通过Ctrl+鼠标左键点中目标多行（可不连续），产生多光标，然后按快捷键Ctrl+Alt+“1”或“-”或“[”或“]”，可跳行设置列表符，尤其是有序列表，数字也会跳行加1 按Alt+鼠标选中行首那列（列选择），这样每行行首都有光标，然后再键入或删除列表符即可批量操作 选中多行，按快捷键Ctrl+Shift+\（其实就是Ctrl+|），可以在每行行首添加一个光标 引用列表 引用1引用2快捷键：Ctrl+Alt+Shift+.智能双击：双击&gt;号可选中整段引用列表智能回车：行尾回车或行中Ctrl+Enter强制换行后会自动续列表；连续按回车会清除列表符；在列表符后回车或行尾Shift+回车，上一行留出列表符 文字样式语法加粗 【快捷键：Ctrl+B，支持多光标；Emmet：b后敲Tab】加粗2_倾斜【Emmet：i后敲Tab；前后包围：选中文字按Ctrl+\是在选区两侧添加光标，可以继续输入】倾斜删除线```123456789101112131415161718192021222324252627282930313233343536373839404142包围插入：先选中文字内容，然后按_*~`等符号，会自动在2侧加包围智能双击：双击语法区前面的定义符号，选中包含定义符的整段文字去包围：选中整段文字后，按Ctrl+Shift+]，可去除2侧包围符号引号括号虽然不属于markdown语法，但也支持相同的包围、选择、去包围操作。引号括号智能双击选择时略特殊的是：双击引号括号内侧，选中引号括号里的内容(不含引号括号)；按下Alt+双击引号括号内侧，则选中包含符号的整段文字HBuilderX还支持以下对2侧文本高效处理的手段1. 选中文字按Ctrl+\是在选区两侧添加光标，可以继续输入~~，会在2侧同时输入2. 向2侧扩大选择：【Win:Alt+Shit+→ 、Mac:Ctrl++Shit+→】；由2侧向内减少选择：【Win:Alt+Shit+← 、Mac:Ctrl++Shit+←】[链接文字](http://dcloud.io)1. Emmet：a后敲Tab2. 打开链接：Alt+鼠标单击；如果是本地文件，可通过Shift+Alt+单击，在另一分栏打开文件3. 智能粘贴：粘贴URL会自动变成超链接格式；粘贴本地文件进来也会自动创建引用链接4. 智能双击：双击语法区开头，即[左侧，选中包含定义符的整段文字![图片描述文字](logo.jpg)1. Emmet：img后敲Tab2. 智能粘贴：粘贴剪切板里的图形时会自动保存为本md文档的附件；删除文档中的图片语法，保存md文档时会自动删除对应的图片附件；粘贴图片文件时自动变成链接引用格式；3. 悬浮预览：鼠标移到图片语法上，本地图片会自动显示出来4. 智能双击：双击语法区开头，即!左侧，选中包含定义符的整段文字# 表格 | | | ||-- |-- |-- || | | || | | |1. Emmet：table3*3后敲Tab，表示生成3行3列的表格，行首生效2. md表格对齐是传统md的痛点，按下Ctrl+K可以自动整理表格格式（暂未兼容不同缩放模式和字体的情况）3. 支持从excel、wps、word、number的表格中复制粘贴表格进来（不支持合并单元格和单元格换行）# 分割线------------- 【Emmet：hr后敲Tab】*************=============# 代码区``` javascript var a = document Emmet：code后敲Tab，行首生效智能双击：双击语法区开头，即!左侧，选中包含定义符的整段文字支持代码直接高亮着色，这应该是只有HBuilderX才有的功能。注意需要在代码区开头指定语言类型 注释 快捷键：Ctrl+/智能双击：双击注释首尾的定义符，选中整段注释 其他emmet快捷输入day后敲Tab，当前日期。注意day需在行首或前面有空格time后敲Tab，当前时间。注意time需在行首或前面有空格 文档结构图文章很长时，word里有文档结构图，HBuilderX也有。菜单视图-文档结构图，快捷键Alt+W(mac是ctrl+W)，轻松管理长文档 运行、预览和打印PDF对md文件点工具栏或菜单里的浏览器运行，可以使用外部浏览器预览此md文件，会自动渲染为HTML。点右上角的预览【快捷键Alt+p】，可在HBuilderX右侧预览该md文档的HTML渲染结果。在浏览器中点打印，选择打印到PDF，可将md输出为PDF格式。（注意在打印选项里去掉页眉页脚） 其他常用但你可能不知道的快捷操作技巧 Ctrl+鼠标左键添加多光标，然后敲字或粘贴，可批量处理。Ctrl+鼠标左键拖选，可选中多个选区。 Ctrl+鼠标右键删除多光标 不选内容按Ctrl+C或X可复制或剪切整行 选中2个选区后，按Ctrl+Shift+X，可互换选区内容。如无选区，只是2个光标，则互换2行 Ctrl+上下键可上下移动行 Ctrl+Insert可重复插入当前行，如果有选中内容，可重复插入选中内容 Ctrl+Shift+K可合并多行（是格式化Ctrl+K的反操作） 删除 按Ctrl+D可删除选中行，支持多光标 Shift+Del删除到行尾 Shift+Backspace删除到行首 选择 Ctrl+E选相同词(mac是Command+D)，连续按可选中多词进一步操作，比替换更方便 Ctrl+L可连选多行，Ctrl+Shift+L也是选择行，但不选行首尾的空白字符 Ctrl+=可逐级放大选区 双击标题、列表符可选中相应段落 双击英文引号、括号内侧，可选中内部内容 双击缩进符，可选中同缩进段落 双击连字符比如-或_，可选中相连的词，比如双击这里试试，uni-app 查找 Ctrl+P查找文件 Ctrl+Alt+F可在当前目录的所有文档中搜索指定关键字(mac是Command+Shift+f) 选中文字按F3，查找下一个，Shift+F3找上一个 云同步：HBuilderX+markdown用于云同步笔记的技巧，请参考http://ask.dcloud.net.cn/article/13097 都学会了吗？markdown语法其实很简单，认真学半小时就能掌握。HBuilderX的极客操作则需要不停反复练习，熟练掌握这些技巧，你将成为高效极客！]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux虚拟机中搭建Hadoop分布式环境]]></title>
    <url>%2F2019%2F10%2F30%2Fhadoop-install%2F</url>
    <content type="text"><![CDATA[学习大数据的过程中，避免不了，需要有大数据的环境，没有，我们就需要自己动手搭建。以下内容是在安装好linux虚拟机的基础上部署Hadoop，并搭建Hadoop分布式环境。一般情况下安装好虚拟机后，需要配置网络静态IP，并设置网卡自动启动，虚拟机才能连接网络。具体设置如下： Linux设置静态IP步骤如下1.先输入命令【route -n】查看路由地址并记下如果命令route提示“bash: route: command not found”使用yum命令安装，仅限Center OSyum install net-tools2.然后找到网卡配置文件编辑【cd /etc/sysconfig/network-scripts/】在文件中添加一下配置IPADDR=192.168.50.130 #希望的ip地址NETMASK=255.255.255.0 #默认GATEWAY=192.168.50.2 #之前记下的网关地址DNS1=192.168.50.2 #和网关一样然后把以下属性求改为如下ONBOOT=”yes” # 开机启动BOOTPROTO=”static” #设置为静态的3.service network restart Hadoop安装环境配置如下一、系统主机名称修改1.使用sudo vi /etc/sysconfig/network 打开配置文件，根据实际情况设置该服务器的机器名，新机器名在重启后生效NETWORKING=yesNETWORKING_IPV6=noHOSTNAME=master2.设置Host映射文件设置IP地址与机器名的映射，设置信息如下：127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.113.129 master192.168.113.130 node01192.168.113.131 node02192.168.113.132 node033.重启机器 reboot4.测试主机名称是否修改成功 二、下载对应系统版本的JDK安装包具体下载从Oracle官网下载相应的版本。下载地址：https://www.oracle.com/technetwork/java/javase/archive-139210.html 三、安装Java JDK1.查看linux下已经安装的JDKrpm -qa | grep java结果：python-javapackages-3.4.1-11.el7.noarchjava-1.8.0-openjdk-headless-1.8.0.161-2.b14.el7.x86_64tzdata-java-2018c-1.el7.noarchjava-1.7.0-openjdk-1.7.0.171-2.6.13.2.el7.x86_64java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64javapackages-tools-3.4.1-11.el7.noarchjava-1.7.0-openjdk-headless-1.7.0.171-2.6.13.2.el7.x86_642.卸载存在的JDKyum remove *openjdk*3.上传下载的JDK安装包rz -f将JDK安装包上传到 /usr/local/java 目录下（该目录一般为JDK的标准安装目录，也可使用whereis java 或 find -name java 查看本地的java目录）4.解压上传的JDK安装包tar -zxvf jdk-8u221-linux-x64.tar.gz5.配置环境变量vim /etc/profile6.在最前面加上 (JAVA_HOME的值为jdk的解压目录)export JAVA_HOME=/usr/local/java/jdk1.8.0_221export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH7.执行profile文件,使环境变量生效source /etc/profile8.验证JDK是否安装成功java -version出现如下信息则安装成功：java version “1.8.0_221”Java(TM) SE Runtime Environment (build 1.8.0_221-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.221-b11, mixed mode) 四、关闭系统防火墙在Hadoop安装过程中需要关闭防火墙和SElinux，否则会出现异常1.CentOS6关闭防火墙使用以下命令，//临时关闭service iptables stop//禁止开机启动chkconfig iptables offCentOS7中若使用同样的命令会报错，stop iptables.serviceFailed to stop iptables.service: Unit iptables.service not loaded.这是因为CentOS7版本后防火墙默认使用firewalld，因此在CentOS7中关闭防火墙使用以下命令，//临时关闭systemctl stop firewalld//禁止开机启动systemctl disable firewalld结果如下：Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.当然，如果安装了iptables-service，也可以使用下面的命令，yum install -y iptables-services//关闭防火墙service iptables stopRedirecting to /bin/systemctl stop iptables.service//检查防火墙状态service iptables statusRedirecting to /bin/systemctl status iptables.serviceiptables.service - IPv4 firewall with iptables Loaded: loaded (/usr/lib/systemd/system/iptables.service; disabled; vendor preset: disabled) Active: inactive (dead)2.关闭SElinux使用getenforce命令查看是否关闭修改/etc/selinux/config 文件将SELINUX=enforcing改为SELINUX=disabled，执行该命令后重启机器生效reboot 五、更新OpenSSLCentOS自带的OpenSSL存在bug，使用如下命令进行更新：yum update openssl 六、克隆配置好网络和安装好JDK的虚拟机（为Hadoop集群做准备）克隆三个节点的虚拟机 node01、node02、node03 然后分别开启虚拟机，配置各个虚拟机的静态IP 七、配置SSH免密登录(直接看第八步)1.使用sudo vi /etc/ssh/sshd_config，打开sshd_config配置文件，开放三个配置，添加到文件内容首部，如下所示：RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys2.重启SSHD服务sudo service sshd restart3.使用如下命令生成 私钥和公钥ssh-keygen -t rsa4.cd ~/.ssh 目录会发现生成了 id_rsa 和 id_rsa.pub 两个秘钥。其中.pub 为公钥。5.把公钥命名为authorized_keys，使用命令如下：cp id_rsa.pub authorized_keys6.使用如下设置authorized_keys读写权限sudo chmod 400 authorized_keys7.测试免密登录 ssh master，此处登录到本机8.配置master免密码登录各个节点将master节点上的authoized_keys远程传输到node01和node02、node03 的~/.ssh/目录下使用一下命令将生成的公钥 authorized_keys 分发到每个节点的 .ssh目录注意：此处会发生.ssh 目录不存在的情况，如果提示.ssh目录不存在，则执行 ssh localhost（主机名） 命令即可！原因： .ssh 是记录密码信息的文件夹，如果没有登录过root的话，就没有 .ssh 文件夹，因此登录 localhost ，并输入密码就会生成了。scp ~/.ssh/authorized_keys root@node01:~/.ssh/scp ~/.ssh/authorized_keys root@node02:~/.ssh/scp ~/.ssh/authorized_keys root@node03:~/.ssh/9.测试免密登录输入ssh node01即可登录到node01节点，输入exit退出当前登录 设置SSH免密钥（最新）每台机器间都要设置免密登录1.关于ssh免密码的设置，要求每两台主机之间设置免密码，自己的主机与自己的主机之间也要求设置免密码。 这项操作可以在admin用户下执行，执行完毕公钥在~/.ssh/id_rsa.pub[admin@node00 ~]# ssh-keygen -t rsa[admin@node00 ~]# ssh-copy-id node01[admin@node00 ~]# ssh-copy-id node02[admin@node00 ~]# ssh-copy-id node032.node1与node2为namenode节点要相互免秘钥 HDFS的HA[admin@node01 ~]# ssh-keygen -t rsa[admin@node01 ~]# ssh-copy-id node00[admin@node01 ~]# ssh-copy-id node02[admin@node01 ~]# ssh-copy-id node033.node2与node3为yarn节点要相互免秘钥 YARN的HA[admin@node02 ~]# ssh-keygen -t rsa[admin@node02 ~]# ssh-copy-id node00[admin@node02 ~]# ssh-copy-id node01[admin@node02 ~]# ssh-copy-id node03… 本机与本机之间设置免密登录将本机生成的id_rsa.pub追加到authorized_keys中cat id_rsa.pub &gt;&gt; authorized_keys 八、添加用户账号在所有的主机下均建立一个账号admin用来运行hadoop ，并将其添加至sudoers中[root@node00 ~]# useradd admin 添加用户通过手动输入修改密码[root@node00 ~]# passwd admin 更改用户 admin 的密码123456 passwd： 所有的身份验证令牌已经成功更新。设置admin用户具有root权限 修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示：1234[root@node00 ~]# vi /etc/sudoers //# Allow root to run any commands anywhere root ALL=(ALL) ALL admin ALL=(ALL) ALL 修改完毕 ：wq！ 保存退出，现在可以用admin帐号登录，然后用命令 su - ，切换用户即可获得root权限进行操作。 九、/opt目录下创建文件夹1）在root用户下创建module、software文件夹12[root@node00 opt]# mkdir module [root@node00 opt]# mkdir software 2）修改module、software文件夹的所有者12[root@node00 opt]# chown admin:admin module [root@node00 opt]# chown admin:admin software 3）查看module、software文件夹的所有者1234[root@node00 opt]# ll total 0 drwxr-xr-x. 5 admin admin 64 May 27 00:24 module drwxr-xr-x. 2 admin admin 267 May 26 11:56 software 十、Hadoop官网上面下载Hadoop安装包，并安装。1.下载地址，此处下载的 hadoop-2.9.2.tar.gz 版本http://hadoop.apache.org/releases.html2.上传到服务器/opt/modulem目录，并解压Hadoop安装包执行如下命令解压：tar -zxvf hadoop-2.9.2.tar.gz解压完成后可以看见目录如下：1234[root@node00 module]# ll 总用量 357860 drwxr-xr-x 9 501 dialout 149 11月 13 2018 hadoop-2.9.2 -rw-r--r-- 1 root root 366447449 10月 31 11:46 hadoop-2.9.2.tar.gz 十一、配置hadoop集群集群部署规划 节点名称 NN1 NN2 DN RN NM node00 NameNode DataNode NodeManager node01 SecondaryNameNode DataNode ResourceManager NodeManager node02 DataNode NodeManager node03 DataNode NodeManager 进入 hadoop-2.9.2/etc/hadoop 目录按照如下顺序配置各个配置文件。（注意：配置文件在hadoop2.7.6/etc/hadoop/下） 修改core-site.xml 修改hadoop-env.sh 修改hdfs-site.xml 修改slaves 修改mapred-env.sh 修改mapred-site.xml 修改yarn-env.sh 修改yarn-site.xml 分发hadoop到节点 配置环境变量下面是每个步骤的详细说明。1.修改core-site.xml[root@node00 hadoop]$ vi core-site.xml 123456789101112&lt;configuration&gt; &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node00:9000&lt;/value&gt; &lt;/property&gt;&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.9.2/data/full/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2.修改hadoop-env.sh12[root@node00 hadoop]$ vi hadoop-env.sh 修改 export JAVA_HOME=/usr/local/java/jdk1.8.0_221 3.修改hdfs-site.xml12345678910111213[admin@node00 hadoop]$ vi hdfs-site.xml&lt;configuration&gt;&lt;!-- 设置dfs副本数，不设置默认是3个,此处我设置为3个 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;!-- 设置secondname的端口 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node01:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.修改slaves12345[admin@node00 hadoop]$ vi slavesnode00node01node02node03 5.修改mapred-env.sh12[admin@node00 hadoop]$ vi mapred-env.sh修改 export JAVA_HOME=/usr/local/java/jdk1.8.0_221 6.修改mapred-site.xml123456789[admin@node00 hadoop]# cp mapred-site.xml.template mapred-site.xml[admin@node00 hadoop]$ vi mapred-site.xml&lt;configuration&gt;&lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7.修改yarn-env.sh12[admin@node00 hadoop]$ vi yarn-env.sh修改 export JAVA_HOME=/usr/local/java/jdk1.8.0_221 8.修改yarn-site.xml12345678910111213[admin@node00 hadoop]$ vi yarn-site.xml&lt;configuration&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node01&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 9.将配置好的hadoop分发到每个节点123[root@node00 module]# scp -r hadoop-2.9.2 node01:/opt/module/[root@node00 module]# scp -r hadoop-2.9.2 node02:/opt/module/[root@node00 module]# scp -r hadoop-2.9.2 node03:/opt/module/ 10.配置环境变量1234567891011121314151617181920212223242526[root@node00 ~]$ sudo vi /etc/profile末尾追加export HADOOP_HOME=/opt/module/hadoop-2.9.2export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin编译生效 source /etc/profile 测试 hadoop命令，出现以下结果说明环境变量配置成功[root@node00 module]# hadoopUsage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar &lt;jar&gt; run a jar file note: please use &quot;yarn jar&quot; to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive classpath prints the class path needed to get the Hadoop jar and the required libraries credential interact with credential providers daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settingsMost commands print help when invoked w/o parameters. 11.将配置好的环境变量分发到各个节点,并逐个编译生效 source /etc/profile123[root@node00 module]# scp /etc/profile node01:/etc/profile[root@node00 module]# scp /etc/profile node02:/etc/profile[root@node00 module]# scp /etc/profile node03:/etc/profile 十二、启动验证集群1.启动集群，如果集群是第一次启动，需要格式化namenode123[root@node00 hadoop-2.9.2]# hdfs namenode -format出现如下信息说明成功：19/10/31 18:02:23 INFO common.Storage: Storage directory /opt/module/hadoop-2.9.2/data/full/tmp/dfs/name has been successfully formatted. 2.启动Hdfs1234567[root@node00 hadoop-2.9.2]# start-dfs.sh Starting namenodes on [node00]node00: starting namenode, logging to /opt/module/hadoop-2.9.2/logs/hadoop-root-namenode-node00.outnode02: starting datanode, logging to /opt/module/hadoop-2.9.2/logs/hadoop-root-datanode-node02.outnode00: starting datanode, logging to /opt/module/hadoop-2.9.2/logs/hadoop-root-datanode-node00.outnode01: starting datanode, logging to /opt/module/hadoop-2.9.2/logs/hadoop-root-datanode-node01.outnode03: starting datanode, logging to /opt/module/hadoop-2.9.2/logs/hadoop-root-datanode-node03.out 注：我第一次启动的时候，发现每个节点的DateNode都没有启动，经查阅资料，多次格式化namenode导致的namenode与datanode之间的不一致（多次格式化，版本不一致）。导致启动失败。 解决：把Hadoop下的log和tmp文件夹删掉、Hadoopdata里面文件删除（没有Hadoopdata文件，则将Hadoop下的dfs下的data里面文件删掉） 过程： 先关闭dfs：./sbin/stop-dfs.sh 删除掉Hadoop下的data里面的东西 删除Hadoop下的logs和temp文件夹 重新进行namenode格式化：hdfs namenode format 重启集群：./sbin/start-dfs.sh 查看进程：jps 123456789$ jps8115 Jps7775 DataNode7681 NameNode7933 SecondaryNameNode 3.启动Yarn： 注意：Namenode和ResourceManger如果不是同一台机器，不能在NameNode上启动 yarn，应该在ResouceManager所在的机器上启动yarn。本次安装Namenode在node00机器上，ResourceManger 配置在 node01机器上。因此需要到node01机器上启动Yarn1234567[root@node01 data]# start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /opt/module/hadoop-2.9.2/logs/yarn-root-resourcemanager-node01.outnode03: starting nodemanager, logging to /opt/module/hadoop-2.9.2/logs/yarn-root-nodemanager-node03.outnode00: starting nodemanager, logging to /opt/module/hadoop-2.9.2/logs/yarn-root-nodemanager-node00.outnode02: starting nodemanager, logging to /opt/module/hadoop-2.9.2/logs/yarn-root-nodemanager-node02.outnode01: starting nodemanager, logging to /opt/module/hadoop-2.9.2/logs/yarn-root-nodemanager-node01.out 4.jps查看各个节点的进程node0012345[root@node00 hadoop-2.9.2]# jps11872 NodeManager12002 Jps11462 NameNode11599 DataNode node01123456[root@node01 data]# jps9554 NodeManager9220 DataNode9895 Jps9450 ResourceManager9326 SecondaryNameNode node021234[root@node02 data]# jps8832 NodeManager8970 Jps8686 DataNode node031234[root@node03 logs]# jps8946 Jps8653 DataNode8798 NodeManager 至此，所有节点都启动成功！5.登录Web查看网页访问地址：http://192.168.113.129:50070/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Centos 虚拟机配置静态IP]]></title>
    <url>%2F2019%2F10%2F30%2Flinux-VM%2F</url>
    <content type="text"><![CDATA[安装Linux虚拟机后，需要设置固定IP用来SSH访问。以便后续的开发工作。 设置步骤如下 先输入命令【route -n】查看路由地址并记下如果命令route提示“bash: route: command not found” 使用yum命令安装，仅限Center OS yum install net-tools 然后找到网卡配置文件编辑【cd /etc/sysconfig/network-scripts/】 在文件中添加一下配置 IPADDR=192.168.50.130 #希望的ip地址 NETMASK=255.255.255.0 #默认 GATEWAY=192.168.50.2 #之前记下的网关地址 DNS1=192.168.50.2 #和网关一样 然后把以下属性求改为如下 ONBOOT=”yes” # 开机启动 BOOTPROTO=”static” #设置为静态的 service network restart]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM中的年轻代 老年代 持久代 GC]]></title>
    <url>%2F2019%2F10%2F17%2Fjava-gc%2F</url>
    <content type="text"><![CDATA[虚拟机中的共划分为三个代：年轻代（Young Generation）、老年代（Old Generation）和持久代（Permanent Generation）。其中持久代主要存放的是Java类的类信息，与垃圾收集要收集的Java对象关系不大。年轻代和年老代的划分是对垃圾收集影响比较大的。 1.年轻代所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。年轻代分三个区。一个Eden区，两个Survivor区(一般而言)。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制“年老区(Tenured)”。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来 对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从第一个Survivor去过来的对象。而且，Survivor区总有一个是空的。同时，根据程序需要，Survivor区是可以配置为多个的（多于两个），这样可以增加对象在年轻代中的存在时间，减少被放到年老代的可能。 2.年老代在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 3.持久代用于存放静态文件，如今Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=进行设置。 4.Scavenge GC一般情况下，当新对象生成，并且在Eden申请空间失败时，就会触发Scavenge GC，对Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。这种方式的GC是对年轻代的Eden区进行，不会影响到年老代。因为大部分对象都是从Eden区开始的，同时Eden区不会分配的很大，所以Eden区的GC会频繁进行。因而，一般在这里需要使用速度快、效率高的算法，使Eden去能尽快空闲出来。 5.Full GC对整个堆进行整理，包括Young、Tenured和Perm。Full GC因为需要对整个对进行回收，所以比Scavenge GC要慢，因此应该尽可能减少Full GC的次数。在对JVM调优的过程中，很大一部分工作就是对于FullGC的调节。有如下原因可能导致Full GC：· 年老代（Tenured）被写满· 持久代（Perm）被写满· System.gc()被显示调用·上一次GC之后Heap的各域分配策略动态变化 老年代的内存溢出，说明在容器下的静态文件过多，比如编译的字节码，jsp编译成servlet，或者jar包。 解决此问题，修改jvm的参数 permsize即可，permsize初始默认为64m。 jvm内存参数-vmargs -Xms128M -Xmx512M -XX:PermSize=64M -XX:MaxPermSize=128M-vmargs 说明后面是VM的参数，所以后面的其实都是JVM的参数了-Xms128m JVM初始分配的堆内存-Xmx512m JVM最大允许分配的堆内存，按需分配-XX:PermSize=64M JVM初始分配的非堆内存-XX:MaxPermSize=128M JVM最大允许分配的非堆内存，按需分配 堆(Heap)和非堆(Non-heap)内存 按照官方的说法：“Java 虚拟机具有一个堆，堆是运行时数据区域，所有类实例和数组的内存均从此处分配。堆是在 Java 虚拟机启动时创建的。”“在JVM中堆之外的内存称为非堆内存(Non-heap memory)”。 可以看出JVM主要管理两种类型的内存：堆和非堆。简单来说堆就是Java代码可及的内存，是留给开发人员使用的；非堆就是JVM留给自己用的，]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PostgreSQL 创建随机生成颜色代码函数]]></title>
    <url>%2F2019%2F07%2F01%2Fpostgres-sql-hexcolor%2F</url>
    <content type="text"><![CDATA[今天想渲染3D地图的时候，随机从数据库生成颜色代码用于前端渲染。自己写了函数生成颜色代码 创建函数的SQL语句如下create function hexColor() returns varchar as $$with cte1 as (select round(random()*15) as hex1, round(random()*15) as hex2, round(random()*15) as hex3, round(random()*15) as hex4, round(random()*15) as hex5, round(random()*15) as hex6),cte2 as (select case when hex1 = 10 then &#39;A&#39;when hex1 = 11 then &#39;B&#39;when hex1 = 12 then &#39;C&#39;when hex1 = 13 then &#39;D&#39;when hex1 = 14 then &#39;E&#39;when hex1 = 15 then &#39;F&#39;else &#39;F&#39; end as hex1h,case when hex2 = 10 then &#39;A&#39;when hex2 = 11 then &#39;B&#39;when hex2 = 12 then &#39;C&#39;when hex2 = 13 then &#39;D&#39;when hex2 = 14 then &#39;E&#39;when hex2 = 15 then &#39;F&#39;else &#39;F&#39; end as hex2h,case when hex3 = 10 then &#39;A&#39;when hex3 = 11 then &#39;B&#39;when hex3 = 12 then &#39;C&#39;when hex3 = 13 then &#39;D&#39;when hex3 = 14 then &#39;E&#39;when hex3 = 15 then &#39;F&#39;else &#39;F&#39; end as hex3h,case when hex4 = 10 then &#39;A&#39;when hex4 = 11 then &#39;B&#39;when hex4 = 12 then &#39;C&#39;when hex4 = 13 then &#39;D&#39;when hex4 = 14 then &#39;E&#39;when hex4 = 15 then &#39;F&#39;else &#39;F&#39; end as hex4h,case when hex5 = 10 then &#39;A&#39;when hex5 = 11 then &#39;B&#39;when hex5 = 12 then &#39;C&#39;when hex5 = 13 then &#39;D&#39;when hex5 = 14 then &#39;E&#39;when hex5 = 15 then &#39;F&#39;else &#39;F&#39; end as hex5h,case when hex6 = 10 then &#39;A&#39;when hex6 = 11 then &#39;B&#39;when hex6 = 12 then &#39;C&#39;when hex6 = 13 then &#39;D&#39;when hex6 = 14 then &#39;E&#39;when hex6 = 15 then &#39;F&#39;else &#39;F&#39; end as hex6h from cte1)select &#39;#&#39; || ltrim(hex1h) || ltrim(hex2h) || ltrim(hex3h) || ltrim(hex4h) || ltrim(hex5h) || ltrim (hex6h) from cte2;$$ LANGUAGE SQL; 调用如下update tablename set color = hexColor();]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库读写分离，网络访问加速策略]]></title>
    <url>%2F2019%2F05%2F20%2Fdatebase-read-write-1%2F</url>
    <content type="text"><![CDATA[数据库主从模式下，要实现读写分离，需要开发数据访问模块，以下记录数据访问模块实现方式。 数据访问模块实现方案 在Mybatis中开发插件,根据关键字（insert,update,select等）将sql分发到不同的数据源 mycat sharding-jdbc网络访问加速，缓存网站资源反向代理+CDN加速，CDN可以理解为各运营商各地服务器。作用： 加快用户访问响应速度 减轻后端服务器的负载压力消息队列MQ RabbitMQ ActiveMQ Kafaka]]></content>
      <categories>
        <category>数据访问模块</category>
      </categories>
      <tags>
        <tag>数据访问模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡的实现方式]]></title>
    <url>%2F2019%2F05%2F20%2FLoad-Balance-1%2F</url>
    <content type="text"><![CDATA[记录一下负载均衡实现的几种方式 软件实现 Apache、Nginx、Reverse-proxy、pWEB 硬件实现F5 DNS负载均衡]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据学习路线]]></title>
    <url>%2F2019%2F05%2F07%2Fbd-learning-path%2F</url>
    <content type="text"><![CDATA[大数据整体学习路线]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leaflet加载各类在线地图]]></title>
    <url>%2F2019%2F04%2F30%2Fgis-leaflet-1%2F</url>
    <content type="text"><![CDATA[Leaflet常用的几种在线地图，加载地址如下，后续将陆续分享使用Leaflet开发WebGIS的相关知识。 地图URLvar baseLayers = { "高德地图": L.tileLayer('http://webrd0{s}.is.autonavi.com/appmaptile?lang=zh_cn&size=1&scale=1&style=8&x={x}&y={y}&z={z}', { subdomains: "1234" }).addTo(map), "高德影像": L.layerGroup([ L.tileLayer('http://webst0{s}.is.autonavi.com/appmaptile?style=6&x={x}&y={y}&z={z}', { subdomains: "1234" }), L.tileLayer('http://webst0{s}.is.autonavi.com/appmaptile?x={x}&y={y}&z={z}&lang=zh_cn&size=1&scale=1&style=8', { subdomains: "1234" }) ]), "天地图": L.layerGroup([ L.tileLayer('http://t{s}.tianditu.cn/DataServer?T=vec_w&X={x}&Y={y}&L={z}', { subdomains: ['0', '1', '2', '3', '4', '5', '6', '7'] }), L.tileLayer('http://t{s}.tianditu.cn/DataServer?T=cva_w&X={x}&Y={y}&L={z}', { subdomains: ['0', '1', '2', '3', '4', '5', '6', '7'] }) ]), "天地图影像": L.layerGroup([ L.tileLayer('http://t{s}.tianditu.cn/DataServer?T=img_w&X={x}&Y={y}&L={z}', { subdomains: ['0', '1', '2', '3', '4', '5', '6', '7'] }), L.tileLayer('http://t{s}.tianditu.cn/DataServer?T=cia_w&X={x}&Y={y}&L={z}', { subdomains: ['0', '1', '2', '3', '4', '5', '6', '7'] }) ]), "天地图地形": L.layerGroup([ L.tileLayer('http://t{s}.tianditu.cn/DataServer?T=ter_w&X={x}&Y={y}&L={z}', { subdomains: ['0', '1', '2', '3', '4', '5', '6', '7'] }), L.tileLayer('http://t{s}.tianditu.cn/DataServer?T=cta_w&X={x}&Y={y}&L={z}', { subdomains: ['0', '1', '2', '3', '4', '5', '6', '7'] }) ]), "Google地图": L.tileLayer('http://mt1.google.cn/vt/lyrs=m@207000000&hl=zh-CN&gl=CN&src=app&x={x}&y={y}&z={z}&s=Galile'), "Google影像": L.layerGroup([ L.tileLayer('http://mt1.google.cn/vt/lyrs=s&hl=zh-CN&gl=CN&x={x}&y={y}&z={z}&s=Gali'), L.tileLayer('http://mt1.google.cn/vt/imgtp=png32&lyrs=h@207000000&hl=zh-CN&gl=cn&x={x}&y={y}&z={z}&s=Galil') ]), "GeoQ ": L.tileLayer('http://map.geoq.cn/ArcGIS/rest/services/ChinaOnlineCommunity/MapServer/tile/{z}/{y}/{x}'), "GeoQ 藏蓝": L.tileLayer('http://map.geoq.cn/ArcGIS/rest/services/ChinaOnlineStreetPurplishBlue/MapServer/tile/{z}/{y}/{x}'), "GeoQ 灰": L.tileLayer('http://map.geoq.cn/ArcGIS/rest/services/ChinaOnlineStreetGray/MapServer/tile/{z}/{y}/{x}') };]]></content>
      <categories>
        <category>GIS</category>
        <category>Leaflet</category>
      </categories>
      <tags>
        <tag>GIS</tag>
        <tag>Leaflet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux虚拟机网卡重启出现 Failed to Start LSB异常]]></title>
    <url>%2F2019%2F04%2F29%2Flinux-q1%2F</url>
    <content type="text"><![CDATA[启动Linux虚拟机发现IPV4地址丢失，配置完IP后重启网卡出现 Failed to start LSB异常 解决方案(执行以下步骤) systemctl stop NetworkManager systemctl disable NetworkManager service network restart]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找.李志]]></title>
    <url>%2F2019%2F04%2F29%2Fmusic-find-lizhi%2F</url>
    <content type="text"><![CDATA[即将三十而立的年纪，走在人生的十字路口，不停的寻找生命的意义。 大帅寻找 我在恶心的世界里，寻找一个像你的人在每个想你的夏天里，等另一种感情我在失败的生活里，寻找一个爱我的人我的悲伤，浪漫和幻想，不对她说起我再也不会把自己，愚蠢的交给过去我的生活和我的想法，从此相隔了万里我整夜整夜地失眠，不是为了和谁再相见曾经爱你的每一条街，是我新鲜生活的起点我在陌生的感动里，寻找一个像你的人就算像过去一样被误解，不快乐又如何我在干裂的春天里，寻找一个平凡的人她的善良，甜蜜和阳光，陪伴我自己我再也不会把自己，彻底的交给一个人我的理想就像这黑夜，一分一秒的断裂我一天一天地发呆，不是为了酝酿些什么真情早已经被他们毁灭，还有什么不能去拒绝是否你也在沉默里，寻找一个像我的人在每个想我的季节里，和他们在一起]]></content>
      <categories>
        <category>Music</category>
      </categories>
      <tags>
        <tag>Music</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搞清楚Spark、Storm、MapReduce的区别]]></title>
    <url>%2F2019%2F04%2F29%2Fbd-spark-mapreduce%2F</url>
    <content type="text"><![CDATA[很多初学者在刚刚接触大数据的时候会有很多疑惑，比如对MapReduce、Storm、Spark三个计算框架的理解经常会产生混乱。哪一个适合对大量数据进行处理？哪一个又适合对实时的流数据进行处理？又该如何来区分他们呢？我对比整理了这3个计算框架的基本知识，大家可以了解一下以便对这个3个计算框架有一个整体的认识。 1. MapReduce-分布式离线计算框架主要适用于大批量的集群任务，由于是批量执行，故时效性偏低。原生支持 Java 语言开发 MapReduce ，其它语言需要使用到 Hadoop Streaming 来开发。 2. Spark-快速通用的计算引擎Spark 是专为大规模数据处理而设计的快速通用的计算引擎，其是基于内存的迭代式计算。Spark 保留了MapReduce 的优点，而且在时效性上有了很大提高，从而对需要迭代计算和有较高时效性要求的系统提供了很好的支持。开发人员可以通过Java、Scala或者Python等语言进行数据分析作业编写，并使用超过80种高级运算符。Spark与HDFS全面兼容，同时还能与其它Hadoop组件—包括YARN以及HBase并行协作。Spark可以被用于处理多种作业类型，比如实时数据分析、机器学习与图形处理。多用于能容忍小延时的推荐与计算系统。 3. Storm-分布式实时大数据处理系统Storm是一个分布式的、可靠的、容错的流式计算框架。Storm 一开始就是为实时处理设计，因此在实时分析/性能监测等需要高时效性的领域广泛采用。Storm在理论上支持所有语言，只需要少量代码即可完成适配。Storm把集群的状态存在Zookeeper或者本地磁盘，所以后台进程都是无状态的（不需要保存自己的状态，都在zookeeper上），可以在不影响系统健康运行的同时失败或重启。Storm可应用于–数据流处理、持续计算（持续地向客户端发送数据，它们可以实时的更新以及展现数据，比如网站指标）、分布式远程过程调用（轻松地并行化CPU密集型操作）。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谁明浪子心]]></title>
    <url>%2F2019%2F04%2F12%2Fmusic-wangjie%2F</url>
    <content type="text"><![CDATA[愿一生勇敢且温柔，有爱可寻亦有梦可追。 可以笑的话不会哭可找到知己哪会孤独偏偏我永没有遇上问我一身足印的风霜怎可结束可以爱的话不退缩可相知的心哪怕追逐可惜每次遇上热爱无法使我感觉我终于遇上幸福你说爱我等于要把我捕捉实在无法担起这一种爱在这夜我又再度漂泊你的痴情请勿继续请你收起一切相信这晚是结局听说太理想的恋爱总不可接触我却哪管千山走遍亦要设法去捕捉听说太理想的一切都不可接触我再置身寂寞路途在那里会有幸福幸福家与国的梦不结束偏偏一颗心抗拒屈服必须要确实领略到就算一生一世也甘心没有局促你说爱我等于要把我捕捉实在无法担起这一种爱在这夜我又再度漂泊你的痴情请勿继续请你收起一切相信这晚是结局听说太理想的恋爱总不可接触我却哪管千山走遍亦要设法去捕捉听说太理想的一切都不可接触我再置身寂寞路途在那里会有幸福听说太理想的恋爱总不可接触我却哪管千山走遍亦要设法去捕捉听说太理想的一切都不可接触我再置身寂寞路途在那里会有幸福幸福]]></content>
      <categories>
        <category>Music</category>
      </categories>
      <tags>
        <tag>Music</tag>
      </tags>
  </entry>
</search>
